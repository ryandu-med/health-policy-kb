import os
import glob
import pickle
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader
from docx import Document

# ==========================================
# âœ… æ™ºèƒ½è·¯å¾„é…ç½® (è‡ªåŠ¨æ‰¾å½“å‰ç›®å½•ä¸‹çš„ data æ–‡ä»¶å¤¹)
# ==========================================
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(CURRENT_DIR, "data")
INDEX_FILE = os.path.join(DATA_DIR, "faiss_index.bin")
META_FILE = os.path.join(DATA_DIR, "kb_data.pkl")


def read_pdf(file_path):
    try:
        reader = PdfReader(file_path)
        text = ""
        for i, page in enumerate(reader.pages):
            if i > 80: break  # é˜²æ­¢æ–‡ä»¶å¤ªé•¿
            text += page.extract_text() or ""
        return chunk_text(text)
    except:
        return []


def read_word(file_path):
    try:
        doc = Document(file_path)
        text = "\n".join([p.text for p in doc.paragraphs])
        return chunk_text(text)
    except:
        return []


def chunk_text(text, size=600, overlap=100):
    if not text: return []
    text = text.replace('\n', ' ').replace('  ', ' ')
    return [text[i:i + size] for i in range(0, len(text), size - overlap) if len(text[i:i + size]) > 50]


def main():
    print(f"=== ğŸš€ å¯åŠ¨ï¼šæ­£åœ¨æ‰«æ data æ–‡ä»¶å¤¹ ===")
    print(f"ğŸ“‚ èµ„æ–™è·¯å¾„: {DATA_DIR}")

    if not os.path.exists(DATA_DIR):
        print(f"âŒ é”™è¯¯ï¼šæ²¡æ‰¾åˆ° data æ–‡ä»¶å¤¹ï¼è¯·ç¡®è®¤ä½ åœ¨ Health_KB ä¸‹åˆ›å»ºäº† data æ–‡ä»¶å¤¹å¹¶æ”¾å…¥äº†æ–‡ä»¶ã€‚")
        return

    print("ğŸ“¥ åŠ è½½ AI æ¨¡å‹...")
    encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    texts = []
    metadata = []

    # æ‰«ææ‰€æœ‰ docx å’Œ pdf
    files = glob.glob(os.path.join(DATA_DIR, "*.*"))
    valid_files = [f for f in files if f.lower().endswith(('.docx', '.pdf'))]

    print(f"\nğŸ“‚ å‘ç° {len(valid_files)} ä¸ªæœ‰æ•ˆæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    for i, f in enumerate(valid_files):
        name = os.path.basename(f)
        print(f"   [{i + 1}/{len(valid_files)}] è¯»å–: {name}")

        chunks = []
        if f.lower().endswith('.docx'):
            chunks = read_word(f)
            type_str = 'word'
        elif f.lower().endswith('.pdf'):
            chunks = read_pdf(f)
            type_str = 'pdf'

        for chunk in chunks:
            texts.append(f"ã€æ–‡ä»¶ã€‘{name}\nå†…å®¹ï¼š{chunk}")
            metadata.append({"source": name, "type": type_str, "content": chunk})

    if texts:
        print(f"\nğŸ’¾ æ­£åœ¨ä¸º {len(texts)} æ¡ç‰‡æ®µç”Ÿæˆç´¢å¼•...")
        embeddings = encoder.encode(texts, normalize_embeddings=True)
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(np.array(embeddings).astype('float32'))

        # ä¿å­˜åˆ° data æ–‡ä»¶å¤¹é‡Œ
        faiss.write_index(index, INDEX_FILE)
        with open(META_FILE, "wb") as f:
            pickle.dump({'texts': texts, 'metadata': metadata}, f)

        print(f"\nâœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {DATA_DIR}")
        print("ğŸ‰ğŸ‰ğŸ‰ æˆåŠŸï¼æ•°æ®å¤„ç†å®Œæˆã€‚è¯·è¿è¡Œ app.pyã€‚")
    else:
        print("âŒ é”™è¯¯ï¼šdata æ–‡ä»¶å¤¹é‡Œæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ Word æˆ– PDF æ–‡ä»¶ï¼")


if __name__ == "__main__":
    main()